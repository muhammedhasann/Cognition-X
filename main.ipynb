{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 23:13:10.399972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 23:13:15.993161: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-31 23:13:15.993249: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-31 23:13:26.695213: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-31 23:13:26.695747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-31 23:13:26.695782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-05-31 23:13:33.179508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-05-31 23:13:33.188155: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-05-31 23:13:33.188282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mohmmad-HP-ProBook-450-G2): /proc/driver/nvidia/version does not exist\n",
      "[nltk_data] Downloading package punkt to /home/mohmmad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/mohmmad/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss 0.8787716627120972\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 329\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom Evaluation Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 329\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 287\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m train_loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Prepare test data for evaluation\u001b[39;00m\n\u001b[1;32m    290\u001b[0m test_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    120\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_input_ids)\n\u001b[0;32m--> 121\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy model for entity extraction\n",
    "nlp = spacy.load('tr_core_news_trf')\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess text by converting to lowercase, removing punctuation, and replacing Turkish characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[çğışöü]', lambda x: {'ç': 'c', 'ğ': 'g', 'ı': 'i', 'ş': 's', 'ö': 'o', 'ü': 'u'}[x.group()], text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "def build_vocabulary(texts):\n",
    "    \"\"\"\n",
    "    Function to build vocabulary from a list of texts.\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and encode text to sequences\n",
    "def text_to_sequence(text, word2idx, max_len=512):\n",
    "    \"\"\"\n",
    "    Function to convert text to a sequence of integers based on a provided word-to-index mapping.\n",
    "    Pads or truncates sequences to a specified maximum length.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    seq = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [word2idx[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    return seq[:max_len]\n",
    "\n",
    "# Define Transformer block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, feedforward_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, feedforward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feedforward_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "        feedforward_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + feedforward_output)\n",
    "        return x\n",
    "\n",
    "# Define BERT-like model\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, feedforward_dim, num_layers, max_len):\n",
    "        super(BERT, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, feedforward_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed_dim, 3)  # For sentiment classification (negative, neutral, positive)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len).unsqueeze(0).expand_as(x).to(x.device)\n",
    "        x = self.token_embeddings(x) + self.position_embeddings(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)  # Average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device, epochs=10):\n",
    "    \"\"\"\n",
    "    Function to train the model with specified number of epochs and return training loss history.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch_input_ids, batch_labels = batch\n",
    "            batch_input_ids, batch_labels = batch_input_ids.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_input_ids)\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Step {step}, Loss {loss.item()}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch} - Average Loss: {avg_loss}\")\n",
    "    \n",
    "    return train_loss_history\n",
    "\n",
    "# Evaluation function with additional metrics\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Function to evaluate the model and return F1 score, accuracy, precision, and recall.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_input_ids, batch_labels = batch\n",
    "            batch_input_ids, batch_labels = batch_input_ids.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_input_ids)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, preds, average='weighted')\n",
    "    \n",
    "    return f1, accuracy, precision, recall\n",
    "\n",
    "# Print evaluation metrics\n",
    "def print_evaluation(f1, accuracy, precision, recall):\n",
    "    \"\"\"\n",
    "    Function to print evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(f\"Weighted F1 Score: {f1}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "# Plot training loss history\n",
    "def plot_training_history(train_loss_history):\n",
    "    \"\"\"\n",
    "    Function to plot the training loss history.\n",
    "    \"\"\"\n",
    "    plt.plot(train_loss_history)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.show()\n",
    "\n",
    "# Predict entities and sentiments\n",
    "def predict_entities_and_sentiments(text, model, word2idx, device):\n",
    "    \"\"\"\n",
    "    Function to predict entities and sentiments from a given text using a pre-trained model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    clean_text = preprocess_text(text)\n",
    "    input_ids = torch.tensor([text_to_sequence(clean_text, word2idx)], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    entities = [ent.text for ent in nlp(text).ents]\n",
    "    sentiments = [\"olumsuz\" if p == 0 else \"nötr\" if p == 1 else \"olumlu\" for p in preds]\n",
    "    \n",
    "    result = {\n",
    "        \"entity_list\": entities,\n",
    "        \"results\": [{\"entity\": ent, \"sentiment\": sentiments[0]} for ent in entities]\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Custom evaluation function\n",
    "def custom_evaluation(predicted_entities, predicted_sentiments, true_entities, true_sentiments):\n",
    "    \"\"\"\n",
    "    Function to evaluate the predicted entities and sentiments against true entities and sentiments.\n",
    "    \"\"\"\n",
    "    entity_score = 0.65 if set(predicted_entities) == set(true_entities) else 0\n",
    "    sentiment_score = 0.35 if set(predicted_sentiments) == set(true_sentiments) else 0\n",
    "    total_score = entity_score + sentiment_score\n",
    "    return total_score\n",
    "\n",
    "# Save model\n",
    "def save_model(model, path='model.pth'):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(model, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Load model\n",
    "def load_model(path='model.pth'):\n",
    "    \"\"\"\n",
    "    Function to load a trained model from a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = torch.load(path)\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    Function to load and preprocess data from a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "        data['clean_text'] = data['text'].apply(preprocess_text)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data('data.csv')\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab = build_vocabulary(train_data['clean_text'])\n",
    "    vocab_list = [\"<PAD>\", \"<UNK>\"] + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "    # Convert text to sequences\n",
    "    train_data['input_ids'] = train_data['clean_text'].apply(lambda x: text_to_sequence(x, word2idx))\n",
    "    test_data['input_ids'] = test_data['clean_text'].apply(lambda x: text_to_sequence(x, word2idx))\n",
    "\n",
    "    # Initialize model and set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vocab_size = len(word2idx)\n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    feedforward_dim = 512\n",
    "    num_layers = 6\n",
    "    max_len = 512\n",
    "    model = BERT(vocab_size, embed_dim, num_heads, feedforward_dim, num_layers, max_len).to(device)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    input_ids = torch.tensor(list(train_data['input_ids']), dtype=torch.long).to(device)\n",
    "    labels = torch.tensor(list(train_data['label']), dtype=torch.long).to(device)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    train_dataset = TensorDataset(input_ids, labels)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=8)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Train model\n",
    "    train_loss_history = train(model, train_dataloader, optimizer, device, epochs=10)\n",
    "\n",
    "    # Prepare test data for evaluation\n",
    "    test_input_ids = torch.tensor(list(test_data['input_ids']), dtype=torch.long).to(device)\n",
    "    test_labels = torch.tensor(list(test_data['label']), dtype=torch.long).to(device)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_labels)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)\n",
    "\n",
    "    # Evaluate model\n",
    "    f1, accuracy, precision, recall = evaluate(model, test_dataloader, device)\n",
    "    print_evaluation(f1, accuracy, precision, recall)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_loss_history)\n",
    "\n",
    "    # Save model\n",
    "    save_model(model)\n",
    "\n",
    "    # Example prediction\n",
    "    example_text = \"\"\"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır\n",
    "    @Twitch\n",
    "    @Kick_Turkey\n",
    "    gibi canlı yayın platformlarında 360p yayın izlerken donmalar\n",
    "    yaşıyoruz. Başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip\n",
    "    alamadığım hizmeti neden ödeyeyim ?\n",
    "    @Turkcell\"\"\"\n",
    "\n",
    "    example_result = predict_entities_and_sentiments(example_text, model, word2idx, device)\n",
    "    print(f\"Örnek Girdi: {example_text}\")\n",
    "    print(f\"Örnek Çıktı: {example_result}\")\n",
    "\n",
    "    # Example evaluation\n",
    "    true_entities = [\"SuperOnline\", \"Twitch\", \"Kick_Turkey\", \"Başka hiç bir operatörler\", \"Turkcell\"]\n",
    "    true_sentiments = [\"olumsuz\", \"nötr\", \"nötr\", \"olumlu\", \"olumsuz\"]\n",
    "    predicted_entities = example_result[\"entity_list\"]\n",
    "    predicted_sentiments = [res[\"sentiment\"] for res in example_result[\"results\"]]\n",
    "\n",
    "    custom_score = custom_evaluation(predicted_entities, predicted_sentiments, true_entities, true_sentiments)\n",
    "    print(f\"Custom Evaluation Score: {custom_score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
