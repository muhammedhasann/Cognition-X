{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 17:46:41.989304: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 17:46:47.482026: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-31 17:46:47.482061: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-31 17:46:59.184943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-31 17:46:59.185464: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-31 17:46:59.185511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-05-31 17:47:09.588605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-05-31 17:47:09.588639: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-05-31 17:47:09.588670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mohmmad-HP-ProBook-450-G2): /proc/driver/nvidia/version does not exist\n",
      "/home/mohmmad/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss 0.7886448502540588\n",
      "Epoch 1, Step 0, Loss 1.0135451555252075\n",
      "Epoch 2, Step 0, Loss 0.7972089052200317\n",
      "Model saved as 'bert_sentiment_model.pth'\n",
      "Weighted F1 Score: 0.569216757741348\n"
     ]
    }
   ],
   "source": [
    "# ml.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AdamW\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for entity extraction\n",
    "nlp = spacy.load('tr_core_news_trf')\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[çğışöü]', lambda x: {'ç': 'c', 'ğ': 'g', 'ı': 'i', 'ş': 's', 'ö': 'o', 'ü': 'u'}[x.group()], text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "def build_vocabulary(texts):\n",
    "    vocab = Counter()\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and encode text to sequences\n",
    "def text_to_sequence(text, word2idx, max_len=512):\n",
    "    tokens = word_tokenize(text)\n",
    "    seq = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [word2idx[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    return seq[:max_len]\n",
    "\n",
    "# Define Transformer block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, feedforward_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, feedforward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feedforward_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "        feedforward_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + feedforward_output)\n",
    "        return x\n",
    "\n",
    "# Define BERT-like model\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, feedforward_dim, num_layers, max_len):\n",
    "        super(BERT, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, feedforward_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed_dim, 3)  # For sentiment classification (negative, neutral, positive)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len).unsqueeze(0).expand_as(x).to(x.device)\n",
    "        x = self.token_embeddings(x) + self.position_embeddings(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)  # Average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Step {step}, Loss {loss.item()}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            true_labels = batch[1].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return f1\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['clean_text'] = data['text'].apply(preprocess_text)\n",
    "    return data\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data('data.csv')\n",
    "\n",
    "    # Ensure the label column is correct and remap labels to 0, 1, 2 for CrossEntropyLoss\n",
    "    label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    data['label'] = data['label'].map(label_mapping)\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab = build_vocabulary(train_data['clean_text'])\n",
    "    vocab_list = [\"<PAD>\", \"<UNK>\"] + [word for word, count in vocab.items() if count > 1]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "    # Encode texts to sequences\n",
    "    train_data['input_ids'] = train_data['clean_text'].apply(lambda x: text_to_sequence(x, word2idx))\n",
    "    test_data['input_ids'] = test_data['clean_text'].apply(lambda x: text_to_sequence(x, word2idx))\n",
    "\n",
    "    # Initialize model and set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vocab_size = len(word2idx)\n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    feedforward_dim = 512\n",
    "    num_layers = 6\n",
    "    max_len = 512\n",
    "    model = BERT(vocab_size, embed_dim, num_heads, feedforward_dim, num_layers, max_len).to(device)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    input_ids = torch.tensor(list(train_data['input_ids']), dtype=torch.long).to(device)\n",
    "    labels = torch.tensor(list(train_data['label']), dtype=torch.long).to(device)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    dataset = TensorDataset(input_ids, labels)\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=8)\n",
    "\n",
    "    # Initialize optimizer and train model\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    train(model, train_dataloader, optimizer, device)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'bert_sentiment_model.pth')\n",
    "    print(\"Model saved as 'bert_sentiment_model.pth'\")\n",
    "\n",
    "    # Create DataLoader for evaluation\n",
    "    input_ids_test = torch.tensor(list(test_data['input_ids']), dtype=torch.long).to(device)\n",
    "    labels_test = torch.tensor(list(test_data['label']), dtype=torch.long).to(device)\n",
    "    test_dataset = TensorDataset(input_ids_test, labels_test)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)\n",
    "\n",
    "    # Evaluate model\n",
    "    f1 = evaluate(model, test_dataloader, device)\n",
    "    print(f'Weighted F1 Score: {f1}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
